{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tempfile\n",
    "from typing import List, Tuple, Union\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMED_ENTITIES = ['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']\n",
    "EMBEDDING_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "factrueval_logger = logging.getLogger('elmo_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(tokens_file_name: str, spans_file_name: str,\n",
    "                  objects_file_name: str) -> Tuple[List[List[Tuple[str, int, int]]], List[List[int]]]:\n",
    "    texts = []\n",
    "    new_text = []\n",
    "    tokens_dict = dict()\n",
    "    with codecs.open(tokens_file_name, mode='r', encoding='utf-8', errors='ignore') as fp:\n",
    "        line_idx = 1\n",
    "        cur_line = fp.readline()\n",
    "        while len(cur_line) > 0:\n",
    "            err_msg = 'File `{0}`: line {1} is wrong!'.format(tokens_file_name, line_idx)\n",
    "            prep_line = cur_line.strip()\n",
    "            if len(prep_line) > 0:\n",
    "                token_description = prep_line.split()\n",
    "                if len(token_description) != 4:\n",
    "                    raise ValueError(err_msg)\n",
    "                if (not token_description[0].isdigit()) or (not token_description[1].isdigit()) or \\\n",
    "                        (not token_description[2].isdigit()):\n",
    "                    raise ValueError(err_msg)\n",
    "                token_id = int(token_description[0])\n",
    "                if token_id in tokens_dict:\n",
    "                    raise ValueError(err_msg)\n",
    "                new_text.append(token_id)\n",
    "                token_start = int(token_description[1])\n",
    "                token_length = int(token_description[2])\n",
    "                tokens_dict[token_id] = (token_description[-1], 'O', token_start, token_length,\n",
    "                                         (len(texts), len(new_text) - 1))\n",
    "            else:\n",
    "                if len(new_text) == 0:\n",
    "                    raise ValueError(err_msg)\n",
    "                texts.append(copy.copy(new_text))\n",
    "                new_text.clear()\n",
    "            cur_line = fp.readline()\n",
    "            line_idx += 1\n",
    "    if len(new_text) > 0:\n",
    "        texts.append(copy.copy(new_text))\n",
    "        new_text.clear()\n",
    "    spans_dict = dict()\n",
    "    with codecs.open(spans_file_name, mode='r', encoding='utf-8', errors='ignore') as fp:\n",
    "        line_idx = 1\n",
    "        cur_line = fp.readline()\n",
    "        while len(cur_line) > 0:\n",
    "            err_msg = 'File `{0}`: line {1} is wrong!'.format(spans_file_name, line_idx)\n",
    "            prep_line = cur_line.strip()\n",
    "            if len(prep_line) > 0:\n",
    "                comment_pos = prep_line.find('#')\n",
    "                if comment_pos < 0:\n",
    "                    raise ValueError(err_msg)\n",
    "                prep_line = prep_line[:comment_pos].strip()\n",
    "                if len(prep_line) == 0:\n",
    "                    raise ValueError(err_msg)\n",
    "                span_description = prep_line.split()\n",
    "                if len(span_description) != 6:\n",
    "                    raise ValueError(err_msg)\n",
    "                if (not span_description[0].isdigit()) or (not span_description[-1].isdigit()) or \\\n",
    "                        (not span_description[-2].isdigit()):\n",
    "                    raise ValueError(err_msg)\n",
    "                span_id = int(span_description[0])\n",
    "                token_IDs = list()\n",
    "                start_token_id = int(span_description[-2])\n",
    "                n_tokens = int(span_description[-1])\n",
    "                if (n_tokens <= 0) or (start_token_id not in tokens_dict):\n",
    "                    raise ValueError(err_msg)\n",
    "                text_idx = tokens_dict[start_token_id][4][0]\n",
    "                token_pos_in_text = tokens_dict[start_token_id][4][1]\n",
    "                for idx in range(n_tokens):\n",
    "                    token_id = texts[text_idx][token_pos_in_text + idx]\n",
    "                    if token_id not in tokens_dict:\n",
    "                        raise ValueError(err_msg)\n",
    "                    token_IDs.append(token_id)\n",
    "                if span_id not in spans_dict:\n",
    "                    spans_dict[span_id] = tuple(token_IDs)\n",
    "            cur_line = fp.readline()\n",
    "            line_idx += 1\n",
    "    with codecs.open(objects_file_name, mode='r', encoding='utf-8', errors='ignore') as fp:\n",
    "        line_idx = 1\n",
    "        cur_line = fp.readline()\n",
    "        while len(cur_line) > 0:\n",
    "            err_msg = 'File `{0}`: line {1} is wrong!'.format(objects_file_name, line_idx)\n",
    "            prep_line = cur_line.strip()\n",
    "            if len(prep_line) > 0:\n",
    "                comment_pos = prep_line.find('#')\n",
    "                if comment_pos < 0:\n",
    "                    raise ValueError(err_msg)\n",
    "                prep_line = prep_line[:comment_pos].strip()\n",
    "                if len(prep_line) == 0:\n",
    "                    raise ValueError(err_msg)\n",
    "                object_description = prep_line.split()\n",
    "                if len(object_description) < 3:\n",
    "                    raise ValueError(err_msg)\n",
    "                if object_description[1] not in {'LocOrg', 'Org', 'Person', 'Location'}:\n",
    "                    factrueval_logger.warning(err_msg + ' The entity `{0}` is unknown.'.format(object_description[1]))\n",
    "                else:\n",
    "                    span_IDs = []\n",
    "                    for idx in range(2, len(object_description)):\n",
    "                        if not object_description[idx].isdigit():\n",
    "                            raise ValueError(err_msg)\n",
    "                        span_id = int(object_description[idx])\n",
    "                        if span_id not in spans_dict:\n",
    "                            raise ValueError(err_msg)\n",
    "                        span_IDs.append(span_id)\n",
    "                    span_IDs.sort(key=lambda span_id: tokens_dict[spans_dict[span_id][0]][2])\n",
    "                    token_IDs = []\n",
    "                    for span_id in span_IDs:\n",
    "                        start_token_id = spans_dict[span_id][0]\n",
    "                        end_token_id = spans_dict[span_id][-1]\n",
    "                        text_idx = tokens_dict[start_token_id][4][0]\n",
    "                        token_pos_in_text = tokens_dict[start_token_id][4][1]\n",
    "                        while token_pos_in_text < len(texts[text_idx]):\n",
    "                            token_id = texts[text_idx][token_pos_in_text]\n",
    "                            token_IDs.append(token_id)\n",
    "                            if token_id == end_token_id:\n",
    "                                break\n",
    "                            token_pos_in_text += 1\n",
    "                        if token_pos_in_text >= len(texts[text_idx]):\n",
    "                            raise ValueError(err_msg)\n",
    "                    if object_description[1] in {'LocOrg', 'Location'}:\n",
    "                        class_label = 'LOC'\n",
    "                    elif object_description[1] == 'Person':\n",
    "                        class_label = 'PER'\n",
    "                    else:\n",
    "                        class_label = 'ORG'\n",
    "                    tokens_are_used = False\n",
    "                    if tokens_dict[token_IDs[0]][1] != 'O':\n",
    "                        tokens_are_used = True\n",
    "                    else:\n",
    "                        for token_id in token_IDs[1:]:\n",
    "                            if tokens_dict[token_id][1] != 'O':\n",
    "                                tokens_are_used = True\n",
    "                                break\n",
    "                    if not tokens_are_used:\n",
    "                        tokens_dict[token_IDs[0]] = (\n",
    "                            tokens_dict[token_IDs[0]][0], 'B-' + class_label,\n",
    "                            tokens_dict[token_IDs[0]][2], tokens_dict[token_IDs[0]][3],\n",
    "                            tokens_dict[token_IDs[0]][4]\n",
    "                        )\n",
    "                        for token_id in token_IDs[1:]:\n",
    "                            tokens_dict[token_id] = (\n",
    "                                tokens_dict[token_id][0], 'I-' + class_label,\n",
    "                                tokens_dict[token_id][2], tokens_dict[token_id][3],\n",
    "                                tokens_dict[token_id][4]\n",
    "                            )\n",
    "            cur_line = fp.readline()\n",
    "            line_idx += 1\n",
    "    list_of_texts = []\n",
    "    list_of_labels = []\n",
    "    for tokens_sequence in texts:\n",
    "        \n",
    "        new_text = []\n",
    "        new_labels_sequence = []\n",
    "        for token_id in tokens_sequence:\n",
    "            new_text.append((tokens_dict[token_id][0], tokens_dict[token_id][2], tokens_dict[token_id][3]))\n",
    "            new_labels_sequence.append(NAMED_ENTITIES.index(tokens_dict[token_id][1]))\n",
    "        list_of_texts.append(new_text)\n",
    "        list_of_labels.append(new_labels_sequence)\n",
    "    return list_of_texts, list_of_labels\n",
    "\n",
    "\n",
    "def load_data_for_training(data_dir_name: str) -> Tuple[List[List[str]], List[List[int]]]:\n",
    "    names_of_files = sorted(list(filter(lambda it: it.startswith('book_'), os.listdir(data_dir_name))))\n",
    "    if len(names_of_files) == 0:\n",
    "        raise ValueError('The directory `{0}` is empty!'.format(data_dir_name))\n",
    "    if (len(names_of_files) % 6) != 0:\n",
    "        raise ValueError('The directory `{0}` contains wrong data!'.format(data_dir_name))\n",
    "    list_of_all_texts = []\n",
    "    list_of_all_labels = []\n",
    "    for idx in range(len(names_of_files) // 6):\n",
    "        base_name = names_of_files[idx * 6]\n",
    "        point_pos = base_name.rfind('.')\n",
    "        if point_pos <= 0:\n",
    "            raise ValueError('The file `{0}` has incorrect name.'.format(base_name))\n",
    "        prepared_base_name = base_name[:point_pos].strip()\n",
    "        if len(prepared_base_name) == 0:\n",
    "            raise ValueError('The file `{0}` has incorrect name.'.format(base_name))\n",
    "        tokens_file_name = os.path.join(data_dir_name, prepared_base_name + '.tokens')\n",
    "        if not os.path.isfile(tokens_file_name):\n",
    "            raise ValueError('The file `{0}` does not exist!'.format(tokens_file_name))\n",
    "        spans_file_name = os.path.join(data_dir_name, prepared_base_name + '.spans')\n",
    "        if not os.path.isfile(spans_file_name):\n",
    "            raise ValueError('The file `{0}` does not exist!'.format(spans_file_name))\n",
    "        objects_file_name = os.path.join(data_dir_name, prepared_base_name + '.objects')\n",
    "        if not os.path.isfile(objects_file_name):\n",
    "            raise ValueError('The file `{0}` does not exist!'.format(objects_file_name))\n",
    "        list_of_texts, list_of_labels = load_document(tokens_file_name, spans_file_name, objects_file_name)\n",
    "        list_of_all_texts += [list(map(lambda it: it[0], cur_text)) for cur_text in list_of_texts]\n",
    "        list_of_all_labels += list_of_labels\n",
    "    return list_of_all_texts, list_of_all_labels\n",
    "\n",
    "\n",
    "def load_data_for_testing(data_dir_name: str) -> Tuple[List[List[str]], List[Tuple[str, List[List[Tuple[int, int]]]]]]:\n",
    "    names_of_files = sorted(list(filter(lambda it: it.startswith('book_'), os.listdir(data_dir_name))))\n",
    "    if len(names_of_files) == 0:\n",
    "        raise ValueError('The directory `{0}` is empty!'.format(data_dir_name))\n",
    "    if (len(names_of_files) % 6) != 0:\n",
    "        raise ValueError('The directory `{0}` contains wrong data!'.format(data_dir_name))\n",
    "    list_of_all_texts = []\n",
    "    list_of_all_token_bounds = []\n",
    "    for idx in range(len(names_of_files) // 6):\n",
    "        base_name = names_of_files[idx * 6]\n",
    "        point_pos = base_name.rfind('.')\n",
    "        if point_pos <= 0:\n",
    "            raise ValueError('The file `{0}` has incorrect name.'.format(base_name))\n",
    "        prepared_base_name = base_name[:point_pos].strip()\n",
    "        if len(prepared_base_name) == 0:\n",
    "            raise ValueError('The file `{0}` has incorrect name.'.format(base_name))\n",
    "        tokens_file_name = os.path.join(data_dir_name, prepared_base_name + '.tokens')\n",
    "        if not os.path.isfile(tokens_file_name):\n",
    "            raise ValueError('The file `{0}` does not exist!'.format(tokens_file_name))\n",
    "        spans_file_name = os.path.join(data_dir_name, prepared_base_name + '.spans')\n",
    "        if not os.path.isfile(spans_file_name):\n",
    "            raise ValueError('The file `{0}` does not exist!'.format(spans_file_name))\n",
    "        objects_file_name = os.path.join(data_dir_name, prepared_base_name + '.objects')\n",
    "        if not os.path.isfile(objects_file_name):\n",
    "            raise ValueError('The file `{0}` does not exist!'.format(objects_file_name))\n",
    "        list_of_texts, list_of_labels = load_document(tokens_file_name, spans_file_name, objects_file_name)\n",
    "        list_of_all_texts += [list(map(lambda it: it[0], cur_text)) for cur_text in list_of_texts]\n",
    "        list_of_all_token_bounds.append(\n",
    "            (\n",
    "                base_name,\n",
    "                [list(map(lambda it: (it[1], it[2]), cur_text)) for cur_text in list_of_texts]\n",
    "            )\n",
    "        )\n",
    "    return list_of_all_texts, list_of_all_token_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_length_of_data(train_texts, test_texts):\n",
    "    train_maximum_length = len(max(train_texts,key=len))\n",
    "    test_maximum_length = len(max(test_texts,key=len))\n",
    "    return max(train_maximum_length, test_maximum_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_data_to_the_shape(texts, labels, maximum_length):\n",
    "    cast_texts = [x[:min(len(x), maximum_length)] + ['']*max((maximum_length - len(x)),0) for x in texts]\n",
    "    cast_labels = [to_categorical(x[:min(len(x), maximum_length)] + [0]*max((maximum_length - len(x)),0), num_classes=len(NAMED_ENTITIES)) for x in labels]\n",
    "    return cast_texts, cast_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_texts(texts, max_length, embedding_size, batch_size):\n",
    "    embeddings = np.zeros((len(texts), max_length, embedding_size), dtype=np.float32)\n",
    "    steps_per_batch = int(math.ceil(len(texts)/float(batch_size)))                         \n",
    "    with tf.Graph().as_default():\n",
    "        elmo = hub.Module(ELMO_URL, trainable=True)\n",
    "        with tf.Session() as sess:\n",
    "            token_sentences = tf.placeholder(shape=(None, None), dtype=tf.string, name='tokens')\n",
    "            token_sentences_lenghts = tf.placeholder(shape=(None,), dtype=tf.int32, name='tokens_length')\n",
    "            embeddings_of_texts = elmo(\n",
    "                inputs={\n",
    "                    'tokens': token_sentences,\n",
    "                    'sequence_len': token_sentences_lenghts\n",
    "                },\n",
    "                signature='tokens',\n",
    "                as_dict=True)[\"elmo\"]\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            for current_position in range(steps_per_batch, len(texts), steps_per_batch):\n",
    "                previous_position = current_position - steps_per_batch\n",
    "                current_texts = texts[previous_position:current_position]\n",
    "                current_embeddings = sess.run(\n",
    "                    embeddings_of_texts,\n",
    "                    feed_dict={\n",
    "                        token_sentences: current_texts,\n",
    "                        token_sentences_lenghts: [len(np.where(np.array(text) != '')[0]) for text in current_texts]\n",
    "                    })\n",
    "                embeddings[previous_position:current_position] = current_embeddings\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transform data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:elmo_lstm:File `data/devset\\book_263.objects`: line 4 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3632.objects`: line 48 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 11 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 12 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 13 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 14 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 15 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 16 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 17 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 18 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 19 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 20 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 21 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3883.objects`: line 25 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3887.objects`: line 2 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3887.objects`: line 6 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3887.objects`: line 9 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3887.objects`: line 33 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3887.objects`: line 34 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3954.objects`: line 13 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3954.objects`: line 27 is wrong! The entity `Project` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3954.objects`: line 29 is wrong! The entity `Facility` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3954.objects`: line 38 is wrong! The entity `Facility` is unknown.\n",
      "WARNING:elmo_lstm:File `data/testset\\book_3954.objects`: line 39 is wrong! The entity `Project` is unknown.\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = load_data_for_training('data/devset')\n",
    "test_data, test_labels = load_data_for_training('data/testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = get_maximum_length_of_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_train_data, cast_train_labels = cast_data_to_the_shape(train_data, train_labels, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(cast_train_data)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get embeddings</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELMO_URL = \"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Users\\Goodman\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Users\\Goodman\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings_from_texts(cast_train_data, MAX_LENGTH, EMBEDDING_SIZE, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(train_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model creating and fitting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional, BatchNormalization, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_recognition_nn(input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation=\"sigmoid\", return_sequences = True, dropout = 0.2), input_shape=(MAX_LENGTH, EMBEDDING_SIZE)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Dense(len(NAMED_ENTITIES), activation='softmax')))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = named_entity_recognition_nn(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1769/1769 [==============================] - 114s 64ms/step - loss: 0.3864 - categorical_accuracy: 0.9281\n",
      "Epoch 2/10\n",
      "1769/1769 [==============================] - 111s 63ms/step - loss: 0.0270 - categorical_accuracy: 0.9934\n",
      "Epoch 3/10\n",
      "1769/1769 [==============================] - 110s 62ms/step - loss: 0.0182 - categorical_accuracy: 0.9949\n",
      "Epoch 4/10\n",
      "1769/1769 [==============================] - 108s 61ms/step - loss: 0.0130 - categorical_accuracy: 0.9962\n",
      "Epoch 5/10\n",
      "1769/1769 [==============================] - 109s 62ms/step - loss: 0.0100 - categorical_accuracy: 0.9970\n",
      "Epoch 6/10\n",
      "1769/1769 [==============================] - 109s 62ms/step - loss: 0.0081 - categorical_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "1769/1769 [==============================] - 109s 61ms/step - loss: 0.0063 - categorical_accuracy: 0.9981\n",
      "Epoch 8/10\n",
      "1769/1769 [==============================] - 108s 61ms/step - loss: 0.0046 - categorical_accuracy: 0.9986\n",
      "Epoch 9/10\n",
      "1769/1769 [==============================] - 108s 61ms/step - loss: 0.0028 - categorical_accuracy: 0.9992\n",
      "Epoch 10/10\n",
      "1769/1769 [==============================] - 108s 61ms/step - loss: 0.0030 - categorical_accuracy: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e353779198>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_embeddings, np.array(cast_train_labels),  epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prediction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_test_data, cast_test_labels = cast_data_to_the_shape(test_data, test_labels, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings_from_texts(cast_test_data, MAX_LENGTH, EMBEDDING_SIZE, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_probs = model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>F1 score</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels = np.array([[np.argmax(word) for word in sentence] for sentence in predicted_test_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_labels = np.array([[np.argmax(word) for word in sentence] for sentence in cast_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208276831999892"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.hstack(true_test_labels), np.hstack(predicted_test_labels),\n",
    "         average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
